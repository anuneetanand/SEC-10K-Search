{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import our libraries\n",
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.setrecursionlimit(10000)\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from bs2json import bs2json\n",
    "import pickle\n",
    "\n",
    "\n",
    "def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "        Replace C1 control characters in the Unicode string s by the\n",
    "        characters at the corresponding code points in Windows-1252,\n",
    "        where possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "        except UnicodeDecodeError:\n",
    "            # No character at the corresponding code point: remove it.\n",
    "            return ''\n",
    "        \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)\n",
    "\n",
    "\n",
    "for file in tqdm(Path('data/data').glob(\"*.txt\")):\n",
    "    try:\n",
    "        with open(file,'r') as f:\n",
    "            soup = BeautifulSoup(f.read(), 'lxml')\n",
    "        done = False\n",
    "        for filing_document in soup.find_all('document'):\n",
    "            document_id = filing_document.type.find(text=True, recursive=False).strip()\n",
    "            if document_id == \"10-K\":\n",
    "                document_sequence = filing_document.sequence.find(text=True, recursive=False).strip()\n",
    "                document_filename = filing_document.filename.find(text=True, recursive=False).strip()\n",
    "                try:\n",
    "                    document_description = filing_document.description.find(text=True, recursive=False).strip()\n",
    "                except:\n",
    "                    document_description = \"\"\n",
    "    \n",
    "                filing_doc_text = filing_document.find('text').extract()\n",
    "                all_thematic_breaks = filing_doc_text.find_all('hr',{'width':'100%'})\n",
    "                all_page_numbers = []\n",
    "                all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "                filing_doc_string = str(filing_doc_text)\n",
    "                if len(all_thematic_breaks) > 0:\n",
    "                    regex_delimiter_pattern = '|'.join(map(re.escape, all_thematic_breaks))\n",
    "                    split_filing_string = re.split(regex_delimiter_pattern, filing_doc_string)\n",
    "                    pages_code = split_filing_string\n",
    "                \n",
    "                elif len(all_thematic_breaks) == 0:\n",
    "                    split_filing_string = all_thematic_breaks\n",
    "                    pages_code = [filing_doc_string]\n",
    "\n",
    "                document_pages = pages_code\n",
    "                pages_length = len(pages_code)\n",
    "                repaired_pages = {}\n",
    "                normalized_text = {}\n",
    "                for index, page in enumerate(document_pages):\n",
    "                    page_soup = BeautifulSoup(page,'lxml')\n",
    "                    try:\n",
    "                        page_text = page_soup.html.body.get_text(' ',strip = True)\n",
    "                    except : \n",
    "                        page_text = \"\"\n",
    "                    try:\n",
    "                        page_text_norm = restore_windows_1252_characters(unicodedata.normalize('NFKD', page_text)) \n",
    "                    except:\n",
    "                        page_text_norm = \"\"\n",
    "                    try:\n",
    "                        page_text_norm = page_text_norm.replace('  ', ' ').replace('\\n',' ')\n",
    "                    except:\n",
    "                        page_text_norm = \"\"\n",
    "\n",
    "                    page_number = index + 1\n",
    "                    normalized_text[page_number] = page_text_norm\n",
    "                    repaired_pages[page_number] = page_soup\n",
    "\n",
    "                pages_normalized_text = normalized_text\n",
    "\n",
    "                # add the repaired html code back to the document dictionary\n",
    "                pages_code = repaired_pages\n",
    "                name = str(file).split('/')[-1].split('.')[0]\n",
    "                with open(f'data/htmls/{name}.html','w') as f:\n",
    "                    f.write(str(pages_code[1]))\n",
    "                with open(f'data/normalised/{name}.txt','w') as f:\n",
    "                    f.write(pages_normalized_text[1])\n",
    "                done = True\n",
    "                break\n",
    "        if done == False:\n",
    "            with open('not_done.txt','a') as f:\n",
    "                f.write(file+\" 10-k not found\\n\")\n",
    "\n",
    "    except:\n",
    "        with open('not_done.txt','a') as f:\n",
    "            f.write(str(file)+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/htmls/175.html\",'r') as f:\n",
    "    soup = BeautifulSoup(f.read(), 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs2json import bs2json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "already_done = [str(i).split('/')[-1].split('.')[0] for i in Path('data/text_only').glob(\"*.txt\")]\n",
    "converter = bs2json()\n",
    "final = []\n",
    "def f(i):\n",
    "    global final\n",
    "    if type(i) == dict:\n",
    "        for j in i:\n",
    "            if type(i[j]) == dict or type(i[j]) == list:\n",
    "                f(i[j])\n",
    "            elif j == \"text\" and i[j] != \"\":\n",
    "                final.append(i[j])\n",
    "                \n",
    "    elif type(i) == list:\n",
    "        for j in i:\n",
    "            if type(j) == dict or type(j) == list:\n",
    "                f(j)\n",
    "\n",
    "for i in tqdm(Path('data/htmls').glob(\"*.html\")):\n",
    "    name = str(i).split('/')[-1].split('.')[0]\n",
    "    if name not in already_done:\n",
    "        with open(i,'r') as f1:\n",
    "            soup = BeautifulSoup(f1.read(), 'lxml')\n",
    "\n",
    "\n",
    "        js = converter.convert(soup.find(''))\n",
    "        final = []\n",
    "        f(js)\n",
    "        with open(f\"data/text_only/{name}.txt\",'w') as f2:\n",
    "            for j in final:\n",
    "                f2.write(j + \"\\n\")\n",
    "    else:\n",
    "        print('working')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
