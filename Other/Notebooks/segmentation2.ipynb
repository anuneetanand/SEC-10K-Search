{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from helper import *\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import codecs\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "def f1(x):\n",
    "    global final\n",
    "    try:\n",
    "        for i in x.children:\n",
    "            f1(i)\n",
    "    except AttributeError as error:\n",
    "        final.append(x)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def clean(text):    \n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = ' '.join([w for w in word_tokenize(text) if not w in stop_words])\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(text) if not w in stop_words])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [i for i in Path('../data/htmls/').glob(\"*.html\")]\n",
    "random.shuffle(files)\n",
    "Postings = {}\n",
    "Index = OrderedDict()\n",
    "Sentences = {}\n",
    "\n",
    "for file in tqdm(files):\n",
    "    with open(file,'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'lxml')\n",
    "    final = []\n",
    "    f1(soup)\n",
    "    Tokens = word_tokenize(clean(' '.join(final)))\n",
    "    ID = str(file).split('/')[-1].split('.')[0]   \n",
    "    Sentences[ID] = final[:]\n",
    "    for t in Tokens:\n",
    "        if t in Postings:\n",
    "            if ID in Postings[t]:\n",
    "                Postings[t][ID] += 1\n",
    "            else:\n",
    "                Postings[t][ID] = 1\n",
    "        else:\n",
    "            Postings[t] = {}\n",
    "            Postings[t][ID] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Terms = list(Postings.keys())\n",
    "Terms.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Terms:\n",
    "    Index[i] = Postings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Index,open('../data/index2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Index.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index['aac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sentences.json','w') as f:\n",
    "    json.dump(Sentences,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
